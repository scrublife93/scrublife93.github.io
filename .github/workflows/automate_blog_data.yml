name: Automate Blog Data

on:
  schedule:
    - cron: '0 4 * * *' # Runs at 04:00 UTC every day
  workflow_dispatch: # Allows manual trigger

jobs:
  sync_burgers:
    runs-on: ubuntu-latest
    permissions:
      contents: write # Required to push changes
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
        with:
          fetch-depth: 0 # Required for rebase

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'
          cache: 'pip'

      - name: Install dependencies
        run: pip install -r requirements.txt

      - name: Run Sync Script
        env:
          NOTION_KEY: ${{ secrets.NOTION_KEY }}
          NOTION_DATABASE_ID: ${{ secrets.NOTION_DATABASE_ID }}
        run: python scripts/sync_burgers.py

      - name: Verify Data Integrity
        run: |
          if [ ! -s assets/data/burgers.json ]; then
            echo "Error: burgers.json is empty or missing!"
            exit 1
          fi
          # Optional: Check for minimum length to prevent Nuking data
          LINE_COUNT=$(wc -l < assets/data/burgers.json)
          if [ "$LINE_COUNT" -lt 5 ]; then
             echo "Error: burgers.json seems too short ($LINE_COUNT lines). Potential data loss."
             exit 1
          fi

      - name: Commit and push changes
        run: |
          git config --global user.name 'GitHub Actions'
          git config --global user.email 'actions@github.com'
          
          git add assets/data/burgers.json assets/images/burgers/
          
          # Only commit if there are changes
          if [[ -n $(git status -s) ]]; then
            git commit -m "roboburger: update blog data [skip ci]"
            
            # Rebase to prevent conflicts with recent manual edits
            # MUST be done after commit to avoid "dirty directory" errors
            git pull --rebase
            
            git push
          else
            echo "No changes to commit"
          fi
